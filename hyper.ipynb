{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b96e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb23e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import optuna\n",
    "\n",
    "# ─── 1) SETUP LOGGING ──────────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%Y/%m/%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ─── 2) LOAD & SPLIT EUROPARL EN–ES ────────────────────────────────────────────\n",
    "logger.info(\"Loading Europarl English–Spanish dataset…\")\n",
    "raw = load_dataset(\"europarl_bilingual\", \"en-es\")\n",
    "if \"validation\" not in raw:\n",
    "    logger.info(\"Creating a 10% validation split…\")\n",
    "    split = raw[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    raw = DatasetDict({\n",
    "        \"train\": split[\"train\"],\n",
    "        \"validation\": split[\"test\"],\n",
    "        \"test\": raw.get(\"test\",\n",
    "                        split[\"train\"].train_test_split(test_size=0.2, seed=42)[\"test\"])\n",
    "    })\n",
    "\n",
    "# ─── 3) SUBSAMPLE FOR SPEED ──────────────────────────────────────────────────\n",
    "max_train, max_val = 30_000, 3_000\n",
    "if len(raw[\"train\"]) > max_train:\n",
    "    raw[\"train\"] = raw[\"train\"].select(range(max_train))\n",
    "if len(raw[\"validation\"]) > max_val:\n",
    "    raw[\"validation\"] = raw[\"validation\"].select(range(max_val))\n",
    "\n",
    "# ─── 4) TOKENIZATION ──────────────────────────────────────────────────────────\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "max_len = 128\n",
    "\n",
    "def preprocess(batch, idxs):\n",
    "    logger.info(f\"Tokenizing examples {idxs[0]}–{idxs[-1]}…\")\n",
    "    inputs  = [t[\"en\"] for t in batch[\"translation\"]]\n",
    "    targets = [t[\"es\"] for t in batch[\"translation\"]]\n",
    "    enc = tokenizer(inputs,  max_length=max_len, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        lbl = tokenizer(targets, max_length=max_len, truncation=True, padding=\"max_length\")\n",
    "    enc[\"labels\"] = lbl[\"input_ids\"]\n",
    "    return enc\n",
    "\n",
    "tokenized = raw.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    batch_size=5000,\n",
    "    with_indices=True,\n",
    "    remove_columns=raw[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# ─── 5) DATA COLLATOR ─────────────────────────────────────────────────────────\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"longest\")\n",
    "\n",
    "# ─── 6) MODEL INIT ────────────────────────────────────────────────────────────\n",
    "def model_init():\n",
    "    m = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        tie_encoder_decoder=True,\n",
    "    )\n",
    "\n",
    "    # ─── Enable true seq2seq decoder with cross‐attention:\n",
    "    m.config.decoder.is_decoder        = True\n",
    "    m.config.decoder.add_cross_attention = True\n",
    "\n",
    "    # ─── Special tokens & lengths\n",
    "    m.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "    m.config.eos_token_id           = tokenizer.sep_token_id\n",
    "    m.config.pad_token_id           = tokenizer.pad_token_id\n",
    "    m.config.max_length             = 128\n",
    "    m.config.min_length             = 10\n",
    "    m.config.no_repeat_ngram_size   = 3\n",
    "\n",
    "    return m\n",
    "\n",
    "# ─── 7) HYPERPARAMETER SPACE ─────────────────────────────────────────────────\n",
    "def hp_space(trial: optuna.Trial):\n",
    "    return {\n",
    "        \"learning_rate\":               trial.suggest_loguniform(\"learning_rate\", 1e-6, 5e-5),\n",
    "        # smaller batch‐size choices to avoid OOM\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "            \"per_device_train_batch_size\", [4, 8, 16]\n",
    "        ),\n",
    "        \"weight_decay\":                trial.suggest_uniform(\"weight_decay\", 0.0, 0.3),\n",
    "        \"warmup_steps\":                trial.suggest_int(\"warmup_steps\", 0, 1000),\n",
    "        \"num_train_epochs\":            trial.suggest_categorical(\"num_train_epochs\", [2, 3, 4]),\n",
    "    }\n",
    "\n",
    "# ─── 8) TUNING ARGS ────────────────────────────────────────────────────────────\n",
    "tuning_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./hp_tuning\",\n",
    "    per_device_train_batch_size=8,      # default, overridden in hp_space\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # Use HF’s Torch AdamW\n",
    "    optim=\"adamw_torch\",\n",
    "\n",
    "    # Temporarily disable mixed precision until HPO+AMP bug is fixed\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "# ─── 9) TRAINER & HPO RUN ─────────────────────────────────────────────────────\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init      = model_init,\n",
    "    args            = tuning_args,\n",
    "    train_dataset   = tokenized[\"train\"],\n",
    "    eval_dataset    = tokenized[\"validation\"],\n",
    "    data_collator   = data_collator,\n",
    "    tokenizer       = tokenizer,\n",
    "    compute_metrics = None,  # replace with your BLEU fn if desired\n",
    ")\n",
    "\n",
    "best = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=20,\n",
    "    n_jobs=1,                       \n",
    "    pruner=optuna.pruners.MedianPruner(),\n",
    "    study_name=\"bert_translation_hp\",\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best.hyperparameters)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
